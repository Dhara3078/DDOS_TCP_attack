# -*- coding: utf-8 -*-
"""Copy of Edge_IIot_DDOS_CNN_GRU_UDP_MinMax.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cZPYaAGPXbJjfCDWV30hmpxgO8uNI2xl
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Load data from CSV fils using pandas
df = pd.read_csv("/content/drive/MyDrive/DDOS Attack Detection/Edge-IIot/Edge-Iot-Normal-DDOS_UDP-Sorted.csv")

# Remove columns which have only one unique value because it won't hold any value to the DDoS Attack detection.
zero_variance_cols = [col for col in df.columns if df[col].nunique() == 1]
df = df.drop(columns=zero_variance_cols)
df.columns

not_important_columns = ['ip.src_host', 'ip.dst_host']
df = df.drop(columns=not_important_columns)

len(df.columns)

df.replace([np.inf, -np.inf], np.nan, inplace=True)
df.dropna(inplace=True)

# Separate categorical and numerical columns
categorical_columns = df.select_dtypes(include=['object', 'category']).columns
numeric_columns = df.select_dtypes(include=['number']).columns

print("\nCategorical Columns:", len(categorical_columns))
print(categorical_columns)

print("\nNumerical Columns:", len(numeric_columns))
print(numeric_columns)

# Get unique values for categorical features
for col in categorical_columns:
    print(f"\nUnique values in {col}:")
    print(df[col].value_counts())

df = df[numeric_columns]

from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
X_scaled = scaler.fit_transform(df.drop(columns=['Attack_label']))
y = df['Attack_label'].values

X_scaled.shape

import numpy as np
from sklearn.model_selection import train_test_split

X_seq = []
y_seq = []
seq_length = 20

for i in range(0, len(X_scaled) - seq_length, 1):

    X_seq.append(X_scaled[i:i+seq_length])
    y_seq.append(y[i+seq_length-1])  # use last label in sequence

X_seq = np.array(X_seq)
y_seq = np.array(y_seq)

# Train/test split
x_train, x_test, y_train, y_test = train_test_split(X_seq, y_seq, test_size=0.2, random_state=42)

x_train.shape

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, GRU, Dense, Dropout, BatchNormalization
from tensorflow.keras.losses import BinaryCrossentropy
from tensorflow.keras.optimizers import Adam

model = Sequential([
    Conv1D(4, kernel_size=3, activation='relu', input_shape=(seq_length, X_seq.shape[2])),
    MaxPooling1D(pool_size=2),
    BatchNormalization(),
    Dropout(0.5),
    GRU(2, return_sequences=False),
    Dropout(0.5),
    Dense(1, activation='sigmoid')
])
model.compile(loss=BinaryCrossentropy(), optimizer=Adam(), metrics=['accuracy'])

model.fit(x_train, y_train, epochs=3, batch_size=64, validation_data=(x_test, y_test))

# print confusion matrix

from sklearn.metrics import confusion_matrix

y_pred = model.predict(x_test)
y_pred_classes = (y_pred > 0.5).astype("int32")

conf_matrix = confusion_matrix(y_test, y_pred_classes)

plt.figure(figsize=(8, 6))
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Confusion Matrix')
plt.show()

# calculate accuracy on test dataset with precision and recall

from sklearn.metrics import accuracy_score, precision_score, recall_score

# Calculate evaluation metrics
accuracy = accuracy_score(y_test, y_pred_classes)
precision = precision_score(y_test, y_pred_classes)
recall = recall_score(y_test, y_pred_classes)

print(f"Accuracy on the test set: {accuracy:.4f}")
print(f"Precision on the test set: {precision:.4f}")
print(f"Recall on the test set: {recall:.4f}")

# Save the model
model.save('/content/drive/MyDrive/DDOS Attack Detection/Edge-IIot/cnn_gru_model_UDP_14_aug_minmax_scaling.h5')

print("Model saved successfully to Google Drive.")

# Install the pyswarm package, which is used for Particle Swarm Optimization (PSO)
# PSO is an optimization technique inspired by the social behavior of birds and fish
!pip install pyswarm

# Import necessary modules from TensorFlow Keras for building and training neural networks
from tensorflow.keras.models import Sequential  # Used to create a linear stack of layers for the model
from tensorflow.keras.layers import Conv1D, MaxPooling1D, GRU, Dense, Dropout, BatchNormalization  # Different types of layers for the neural network
from tensorflow.keras.losses import BinaryCrossentropy  # Loss function for binary classification
from tensorflow.keras.optimizers import Adam  # Optimizer for updating model weights

def objective_function(hyperparameters_array):
    """
    Builds, trains, and evaluates a CNN-GRU model with given hyperparameters.

    Args:
        hyperparameters_array (np.ndarray): Numpy array containing hyperparameter values.

    Returns:
        float: Negative of the validation accuracy.
    """
    # Extract the number of convolutional filters from the first element and convert to integer
    conv_filters = int(hyperparameters_array[0])
    # Extract the kernel size for the convolutional layer from the second element and convert to integer
    kernel_size = int(hyperparameters_array[1])
    # Extract the number of GRU units from the third element and convert to integer
    gru_units = int(hyperparameters_array[2])
    # Extract the dropout rate (a float between 0 and 1) from the fourth element
    dropout_rate = hyperparameters_array[3]
    # Extract the learning rate (a small float value) from the fifth element
    learning_rate = hyperparameters_array[4]

    # Print out the hyperparameters being used for this run, so we know what is being tested
    print(f"Evaluating with: conv_filters={conv_filters}, kernel_size={kernel_size}, gru_units={gru_units}, dropout_rate={dropout_rate:.4f}, learning_rate={learning_rate:.6f}")

    # Build the neural network model as a sequence of layers
    model = Sequential([
        # Add a 1D convolutional layer to extract features from the input sequences
        Conv1D(conv_filters, kernel_size=kernel_size, activation='relu', input_shape=(x_train.shape[1], x_train.shape[2])),
        # Add a max pooling layer to reduce the size of the feature maps and keep important information
        MaxPooling1D(pool_size=2),
        # Add batch normalization to help the model train faster and more reliably
        BatchNormalization(),
        # Add dropout to randomly turn off some neurons during training to prevent overfitting
        Dropout(dropout_rate),
        # Add a GRU (Gated Recurrent Unit) layer to learn patterns over time in the sequence data
        GRU(gru_units, return_sequences=False),
        # Add another dropout layer for more regularization
        Dropout(dropout_rate),
        # Add a dense (fully connected) layer with a sigmoid activation to output a probability for binary classification
        Dense(1, activation='sigmoid')
    ])

    # Compile the model: set the loss function, optimizer, and metrics to track
    model.compile(
        loss=BinaryCrossentropy(),  # Use binary cross-entropy loss for binary classification
        optimizer=Adam(learning_rate=learning_rate),  # Use Adam optimizer with the specified learning rate
        metrics=['accuracy']  # Track accuracy during training and validation
    )

    # Train the model on the training data for 1 epoch (one pass through the data)
    # Use a batch size of 64 (process 64 samples at a time)
    # Also evaluate the model on the test data after each epoch
    # Set verbose=1 to show training progress (set to 0 to hide output)
    history = model.fit(x_train, y_train, epochs=1, batch_size=64, validation_data=(x_test, y_test), verbose=1)

    # Print the training history object (contains details about training progress)
    print("HISTORY:", history)
    # Get the last value of validation accuracy from the training history
    validation_accuracy = history.history['val_accuracy'][-1]
    # Print the validation accuracy for this set of hyperparameters
    print(f"Validation Accuracy: {validation_accuracy:.4f}\n")
    # Return the negative of the validation accuracy (because the optimizer tries to minimize the objective)
    return -validation_accuracy

# Define the hyperparameter search space as a dictionary called param_bounds
# Each key in the dictionary is the name of a hyperparameter we want to tune
# The value for each key is a tuple specifying the minimum and maximum values for that hyperparameter

param_bounds = {
    'conv_filters': (16, 64),       # Number of convolutional filters in the Conv1D layer (can be any integer from 16 to 64)
    'kernel_size': (3, 7),          # Size of the filter/kernel in Conv1D (can be any integer from 3 to 7)
    'gru_units': (8, 32),           # Number of units (neurons) in the GRU layer (can be any integer from 8 to 32)
    'dropout_rate': (0.1, 0.5),     # Dropout rate to prevent overfitting (can be any decimal number from 0.1 to 0.5)
    'learning_rate': (0.0001, 0.01) # Learning rate for the Adam optimizer (can be any decimal number from 0.0001 to 0.01)
}

# Print a message to show that the hyperparameter search space has been defined
print("Hyperparameter search space defined:")

# Loop through each hyperparameter and its bounds in the param_bounds dictionary
for param, bounds in param_bounds.items():
    # Print the name of the hyperparameter and its allowed range
    print(f"- {param}: {bounds}")

# The exclamation mark (!) at the beginning tells Jupyter Notebook to run this as a shell command, not Python code.
# 'pip' is the Python package installer, used to install new libraries or packages.
# 'install' is the command to tell pip that we want to install a package.
# 'pyswarm' is the name of the package we want to install. It is a library for Particle Swarm Optimization (PSO),
# which is a technique used to find the best solution to a problem by simulating the movement of a group (swarm) of particles.

!pip install pyswarm

# Import the Particle Swarm Optimization (PSO) function from the pyswarm package
from pyswarm import pso

# Import numpy for numerical operations (already imported above, but shown here for clarity)
import numpy as np

# Define the lower bounds for each hyperparameter
# These are the minimum values that each hyperparameter can take
# The order of the bounds must match the order in which the hyperparameters are passed to the objective function
lower_bounds = [
    param_bounds['conv_filters'][0],    # Minimum number of convolutional filters
    param_bounds['kernel_size'][0],     # Minimum kernel size for Conv1D
    param_bounds['gru_units'][0],       # Minimum number of GRU units
    param_bounds['dropout_rate'][0],    # Minimum dropout rate
    param_bounds['learning_rate'][0]    # Minimum learning rate
]

# Define the upper bounds for each hyperparameter
# These are the maximum values that each hyperparameter can take
upper_bounds = [
    param_bounds['conv_filters'][1],    # Maximum number of convolutional filters
    param_bounds['kernel_size'][1],     # Maximum kernel size for Conv1D
    param_bounds['gru_units'][1],       # Maximum number of GRU units
    param_bounds['dropout_rate'][1],    # Maximum dropout rate
    param_bounds['learning_rate'][1]    # Maximum learning rate
]

# Run the Particle Swarm Optimization (PSO) algorithm to find the best hyperparameters
# pso() will try different combinations of hyperparameters to minimize the objective_function
# - objective_function: the function to minimize (returns negative accuracy, so minimizing means maximizing accuracy)
# - lower_bounds: the minimum values for each hyperparameter
# - upper_bounds: the maximum values for each hyperparameter
# - swarmsize=3: number of candidate solutions (particles) in the swarm (small for demonstration, usually larger)
# - maxiter=5: number of times the swarm will update its positions (iterations)
# - debug=True: print progress and debug information during optimization
best_hyperparameters, _ = pso(
    objective_function,
    lower_bounds,
    upper_bounds,
    swarmsize=3,
    maxiter=5,
    debug=True
)

# The best_hyperparameters are returned as a numpy array
# Convert the array to a dictionary with meaningful names for each hyperparameter
best_hyperparameters_dict = {
    'conv_filters': int(best_hyperparameters[0]),    # Convert to integer (number of filters)
    'kernel_size': int(best_hyperparameters[1]),     # Convert to integer (kernel size)
    'gru_units': int(best_hyperparameters[2]),       # Convert to integer (GRU units)
    'dropout_rate': best_hyperparameters[3],         # Dropout rate (float)
    'learning_rate': best_hyperparameters[4]         # Learning rate (float)
}

# Print the best hyperparameters found by PSO
print("Best hyperparameters found by PSO:")
print(best_hyperparameters_dict)

# Import necessary modules from TensorFlow Keras for building and training the neural network
from tensorflow.keras.models import Sequential  # Used to create a linear stack of layers for the model
from tensorflow.keras.layers import Conv1D, MaxPooling1D, GRU, Dense, Dropout, BatchNormalization  # Different types of layers for the neural network
from tensorflow.keras.losses import BinaryCrossentropy  # Loss function for binary classification
from tensorflow.keras.optimizers import Adam  # Optimizer for updating model weights

# Train the final model with the best hyperparameters

# Extract the best number of convolutional filters from the dictionary
best_conv_filters = best_hyperparameters_dict['conv_filters']
# Extract the best kernel size for the convolutional layer
best_kernel_size = best_hyperparameters_dict['kernel_size']
# Extract the best number of GRU units
best_gru_units = best_hyperparameters_dict['gru_units']
# Extract the best dropout rate
best_dropout_rate = best_hyperparameters_dict['dropout_rate']
# Extract the best learning rate for the optimizer
best_learning_rate = best_hyperparameters_dict['learning_rate']

# Build the final model using the best hyperparameters found by optimization
final_model = Sequential([  # Start building a sequential (layer-by-layer) model
    # Add a 1D convolutional layer to extract features from the input sequences
    Conv1D(best_conv_filters, kernel_size=best_kernel_size, activation='relu', input_shape=(x_train.shape[1], x_train.shape[2])),
    # Add a max pooling layer to reduce the size of the feature maps and keep important information
    MaxPooling1D(pool_size=2),
    # Add batch normalization to help the model train faster and more reliably
    BatchNormalization(),
    # Add dropout to randomly turn off some neurons during training to prevent overfitting
    Dropout(best_dropout_rate),
    # Add a GRU (Gated Recurrent Unit) layer to learn patterns over time in the sequence data
    GRU(best_gru_units, return_sequences=False),
    # Add another dropout layer for more regularization
    Dropout(best_dropout_rate),
    # Add a dense (fully connected) layer with a sigmoid activation to output a probability for binary classification
    Dense(1, activation='sigmoid')
])

# Compile the final model: set the loss function, optimizer, and metrics to track
final_model.compile(
    loss=BinaryCrossentropy(),  # Use binary cross-entropy loss for binary classification
    optimizer=Adam(learning_rate=best_learning_rate),  # Use Adam optimizer with the best learning rate
    metrics=['accuracy']  # Track accuracy during training and validation
)

# Train the final model on the entire training dataset
print("Training the final model with best hyperparameters...")  # Print a message to show training has started
final_model.fit(
    x_train,  # Training features (input data)
    y_train,  # Training labels (target values)
    epochs=3,  # Number of times the model will see the entire training data
    batch_size=64,  # Number of samples processed before the model is updated
    validation_data=(x_test, y_test)  # Data to evaluate the model after each epoch
)  # You can adjust the number of epochs here

# Import the confusion_matrix function from scikit-learn to evaluate classification results
from sklearn.metrics import confusion_matrix

# Use the trained final_model to make predictions on the test data (x_test)
# This returns probabilities (values between 0 and 1) for each sample
y_pred = final_model.predict(x_test)

# Convert the predicted probabilities to class labels (0 or 1)
# If the probability is greater than 0.5, classify as 1 (attack); otherwise, classify as 0 (normal)
y_pred_classes = (y_pred > 0.5).astype("int32")

# Compute the confusion matrix by comparing the true labels (y_test) with the predicted labels (y_pred_classes)
conf_matrix = confusion_matrix(y_test, y_pred_classes)

# Create a new figure for plotting with a specific size (8 inches by 6 inches)
plt.figure(figsize=(8, 6))

# Draw the confusion matrix as a heatmap (colored grid) for better visualization
# annot=True shows the numbers in each cell, fmt='d' means integer format, cmap='Blues' sets the color scheme
sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')

# Label the x-axis as 'Predicted Label'
plt.xlabel('Predicted Label')

# Label the y-axis as 'True Label'
plt.ylabel('True Label')

# Set the title of the plot to 'Confusion Matrix'
plt.title('Confusion Matrix')

# Display the plot on the screen
plt.show()

# Import accuracy_score, precision_score, and recall_score functions from scikit-learn
# These functions help us measure how well our model is performing
from sklearn.metrics import accuracy_score, precision_score, recall_score

# Calculate the accuracy of the model on the test set
# Accuracy is the proportion of correct predictions out of all predictions
accuracy = accuracy_score(y_test, y_pred_classes)

# Calculate the precision of the model on the test set
# Precision tells us, out of all the times the model predicted "attack", how many were actually attacks
precision = precision_score(y_test, y_pred_classes)

# Calculate the recall of the model on the test set
# Recall tells us, out of all the actual attacks, how many did the model correctly identify as attacks
recall = recall_score(y_test, y_pred_classes)

# Print the accuracy value with 4 decimal places
print(f"Accuracy on the test set: {accuracy:.4f}")

# Print the precision value with 4 decimal places
print(f"Precision on the test set: {precision:.4f}")

# Print the recall value with 4 decimal places
print(f"Recall on the test set: {recall:.4f}")

# Save the trained final_model to your Google Drive
# The .save() function writes the model's architecture, weights, and training configuration to a file
# The file will be stored at the specified path in your Google Drive, so you can load and use the model later
final_model.save('/content/drive/MyDrive/DDOS Attack Detection/Edge-IIot/cnn_gru_model_PSO_UDP_14_aug_minmax_scaling.h5')

# Print a message to let you know that the model has been saved successfully
print("Model saved successfully to Google Drive.")

# Import the load_model function from TensorFlow Keras
# This function allows us to load a previously saved neural network model from a file
from tensorflow.keras.models import load_model

# Load the saved model from the specified file path in Google Drive
# The file contains the trained model's architecture and weights
# After loading, we can use this model to make predictions or further analyze it
loaded_model = load_model('/content/drive/MyDrive/DDOS Attack Detection/Edge-IIot/cnn_gru_model_PSO_UDP_14_aug_minmax_scaling.h5')

# Print a message to let us know that the model was loaded successfully
print("Model loaded successfully.")

import tensorflow as tf

# Convert the Keras model to a TensorFlow Lite model
converter = tf.lite.TFLiteConverter.from_keras_model(loaded_model)

# Include Select TF ops and disable experimental lowering of tensor list ops
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS, # Enable TensorFlow Lite built-in operations
    tf.lite.OpsSet.SELECT_TF_OPS # Enable select TensorFlow operations
]
converter._experimental_lower_tensor_list_ops = False # Disable experimental lowering of tensor list ops

tflite_model = converter.convert()

# Save the TensorFlow Lite model to a file
tflite_model_path = '/content/drive/MyDrive/DDOS Attack Detection/Edge-IIot/cnn_gru_model_PSO_UDP_14_aug_minmax_scaling.tflite'
with open(tflite_model_path, 'wb') as f:
    f.write(tflite_model)

print(f"TensorFlow Lite model saved successfully to {tflite_model_path}")

import tensorflow as tf
import numpy as np

# Load the TFLite model
tflite_model_path = '/content/drive/MyDrive/DDOS Attack Detection/Edge-IIot/cnn_gru_model_PSO_UDP_14_aug_minmax_scaling.tflite'
interpreter = tf.lite.Interpreter(model_path=tflite_model_path)

# Allocate tensors
interpreter.allocate_tensors()

# Get input and output tensors
input_details = interpreter.get_input_details()
output_details = interpreter.get_output_details()

# Get the input shape and type
input_shape = input_details[0]['shape']
input_dtype = input_details[0]['dtype']

# Prepare the test data for inference (assuming x_test is available and preprocessed)
# You might need to adjust this based on how your x_test is structured and if it needs reshaping
# For this model, the input shape is (None, 20, 35), so we need to ensure x_test matches this.
# If x_test has a different batch size, the 'None' dimension will adapt.
# Ensure the data type matches the input tensor's dtype (e.g., float32)
test_data_for_inference = x_test.astype(input_dtype)

# Run inference on a subset of the test data (e.g., the first 10 samples) for demonstration
# You can adjust the number of samples as needed
num_test_samples = 100 # Or len(x_test) to run on all test data
test_subset = test_data_for_inference[:num_test_samples]

# Create an array to store the TFLite model predictions
tflite_predictions = []

print(f"Running inference on {num_test_samples} test samples...")

# Iterate over the test subset and perform inference for each sample
for i in range(num_test_samples):
    input_data = test_subset[i:i+1]  # Get one sample (maintaining the batch dimension)

    # Set the input tensor
    interpreter.set_tensor(input_details[0]['index'], input_data)

    # Run inference
    interpreter.invoke()

    # Get the output tensor
    output_data = interpreter.get_tensor(output_details[0]['index'])

    # Append the prediction to the list
    tflite_predictions.append(output_data[0][0]) # Assuming a single output value per sample

# Convert the predictions list to a numpy array
tflite_predictions = np.array(tflite_predictions)

# Print the first few predictions
print("\nFirst 10 TFLite predictions (raw output):")
print(len(tflite_predictions))
print(tflite_predictions[:10])

# If your model output is a probability (like with sigmoid activation), you can convert to binary predictions
# using a threshold (e.g., 0.5)
binary_tflite_predictions = (tflite_predictions > 0.5).astype(int)

print("\nFirst 10 TFLite binary predictions:")
print(binary_tflite_predictions[:10])

# Compare with the true labels for the same subset
true_labels_subset = y_test[:num_test_samples]
print("\nFirst 10 True labels:")
print(true_labels_subset[:10])

# You can further evaluate performance using metrics like accuracy, precision, recall on this subset
from sklearn.metrics import accuracy_score

subset_accuracy = accuracy_score(true_labels_subset, binary_tflite_predictions)
print(f"\nAccuracy on the first {num_test_samples} test samples: {subset_accuracy:.4f}")

# The exclamation mark (!) at the beginning tells Jupyter Notebook to run this as a shell command, not Python code.
# 'pip' is the Python package installer, which is used to add new Python libraries to your environment.
# 'install' is the command that tells pip you want to add a new package.
# 'shap' is the name of the package you want to install. SHAP (SHapley Additive exPlanations) is a library for explaining machine learning models.
!pip install shap

import numpy as np  # Import the numpy library, which is useful for working with arrays and random sampling

# Find the indices (positions) in y_train where the label is 1 (attack)
attack_indices = np.where(y_train == 1)[0]

# Find the indices (positions) in y_train where the label is 0 (normal)
normal_indices = np.where(y_train == 0)[0]

# Randomly select 50 indices from the attack_indices without repeating any index
sampled_attack_indices = np.random.choice(attack_indices, 50, replace=False)

# Randomly select 50 indices from the normal_indices without repeating any index
sampled_normal_indices = np.random.choice(normal_indices, 50, replace=False)

# Combine the sampled attack and normal indices into one array
sampled_indices = np.concatenate((sampled_attack_indices, sampled_normal_indices))

# Use the combined indices to select 100 samples from x_train (features)
x_train_sampled = x_train[sampled_indices]

# Use the same indices to select the corresponding labels from y_train
y_train_sampled = y_train[sampled_indices]

import numpy as np  # Import the numpy library, which helps with array operations and random sampling

# Find the positions (indices) in y_test where the label is 1 (attack)
attack_indices = np.where(y_test == 1)[0]

# Find the positions (indices) in y_test where the label is 0 (normal)
normal_indices = np.where(y_test == 0)[0]

# Randomly pick 25 indices from the attack_indices without repeating any index
sampled_attack_indices = np.random.choice(attack_indices, 25, replace=False)

# Randomly pick 25 indices from the normal_indices without repeating any index
sampled_normal_indices = np.random.choice(normal_indices, 25, replace=False)

# Combine the 25 attack and 25 normal indices into one array (total 50 samples)
sampled_indices = np.concatenate((sampled_attack_indices, sampled_normal_indices))

# Select the 50 samples from x_test using the combined indices
x_test_sampled = x_test[sampled_indices]

# Select the corresponding 50 labels from y_test using the same indices
y_test_sampled = y_test[sampled_indices]

x_test_sampled.shape

y_test_sampled.shape, y_test_sampled

explain_instances_2d.shape

import shap  # Import the SHAP library, which helps explain machine learning model predictions

shap.initjs()  # Initialize JavaScript visualization for SHAP plots in Jupyter Notebook

background_data = x_train_sampled  # Use a small, representative sample of the training data as background for SHAP
background_data_2d = background_data.reshape(background_data.shape[0], -1)  # Flatten each 3D sequence into a 1D vector for SHAP

explain_instances = x_test_sampled  # Select the test samples we want to explain (here, 50 samples)
explain_instances_2d = explain_instances.reshape(explain_instances.shape[0], -1)  # Flatten each test sample from 3D to 1D

def predict_fn_2d(x_2d):
    # This function takes 2D input (samples, features) and reshapes it back to 3D (samples, sequence length, features per step)
    x_3d = x_2d.reshape(x_2d.shape[0], explain_instances.shape[1], explain_instances.shape[2])
    # Use the loaded model to make predictions on the reshaped data
    return loaded_model.predict(x_3d)

# Create a SHAP KernelExplainer object using our prediction function and background data
explainer = shap.KernelExplainer(predict_fn_2d, background_data_2d)

print("Computing SHAP values (this may take some time)...")  # Inform the user that SHAP value computation is starting

# Compute SHAP values for the selected test instances (explains model predictions for each feature)
shap_values = explainer.shap_values(explain_instances_2d)

print("SHAP values computed.")  # Inform the user that SHAP value computation is done

# Create a list called 'features' that contains the names of all numerical columns in the dataset
features = list(numeric_columns)

# Remove the column name "Attack_label" from the 'features' list, because it is the target label, not an input feature
features.remove("Attack_label")

shap_values.shape

import shap  # Import the SHAP library, which helps explain predictions of machine learning models
import numpy as np  # Import numpy for working with arrays and numbers

instance_index = 22  # Choose which test instance (out of 50) we want to explain; here, we pick the 13th one (index 12)

for inex in range(50):
  instance_to_explain = explain_instances_2d[inex]  # Get the feature values (flattened) for the chosen instance

  shap_values_for_waterfall = np.array(shap_values[inex]).flatten()  # Get the SHAP values for this instance and flatten them into a 1D array

  features_for_waterfall = instance_to_explain  # The actual feature values for the chosen instance (already flattened)

  expected_value_scalar = explainer.expected_value  # Get the expected value (average model output over background data)
  if isinstance(expected_value_scalar, np.ndarray):  # Sometimes this is an array, so we convert it to a single number if needed
      expected_value_scalar = expected_value_scalar.item()

  # Create a list of feature names for the flattened input
  # For each time step in the sequence, and for each feature at that time step, add the feature name
  reshaped_feature_names = [features[i] for i in range(explain_instances.shape[1]) for j in range(explain_instances.shape[2])]

  # Create a SHAP Explanation object that holds all the information needed for plotting
  explanation = shap.Explanation(
      values=shap_values_for_waterfall,         # The SHAP values for this instance
      base_values=expected_value_scalar,        # The expected value (baseline prediction)
      data=features_for_waterfall,              # The actual feature values for this instance
      feature_names=reshaped_feature_names      # The names of the features (flattened)
  )

  print(f"Generating waterfall plot for instance {instance_index}...")  # Print a message to show which instance is being explained

  shap.plots.waterfall(explanation)  # Create and display a waterfall plot to show how each feature contributed to the prediction

